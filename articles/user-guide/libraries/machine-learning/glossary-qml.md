---
title: Kvantum gépi tanulási kódtár
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 52c3f69fb99384270a27e57c4f32212d18bee1a4
ms.sourcegitcommit: 6bf99d93590d6aa80490e88f2fd74dbbee8e0371
ms.translationtype: MT
ms.contentlocale: hu-HU
ms.lasthandoff: 08/06/2020
ms.locfileid: "87868899"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="0d7cf-102">Quantum Machine Learning Szószedet</span><span class="sxs-lookup"><span data-stu-id="0d7cf-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="0d7cf-103">Az áramkör-központú kvantummechanika betanítása olyan folyamat, amely számos mozgó részből áll (vagy valamivel nagyobb), mint a hagyományos osztályozók betanítása.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="0d7cf-104">Itt definiáljuk a betanítási folyamat fő fogalmait és összetevőit.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="0d7cf-105">Képzés/tesztelési ütemtervek</span><span class="sxs-lookup"><span data-stu-id="0d7cf-105">Training/testing schedules</span></span>

<span data-ttu-id="0d7cf-106">Az osztályozó képzés kontextusában az *ütemterv* az adatminták egy részhalmazát írja le egy átfogó képzésben vagy tesztelési készletben.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="0d7cf-107">Az ütemtervek meghatározása általában minta-indexek gyűjteménye.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="0d7cf-108">Paraméterek/torzítások pontszámai</span><span class="sxs-lookup"><span data-stu-id="0d7cf-108">Parameter/bias scores</span></span>

<span data-ttu-id="0d7cf-109">A jelölt paraméterek és az osztályozó torzítások miatt a rendszer az *ellenőrzési pontszámot* a kiválasztott ellenőrzési ütemtervhez viszonyítva méri, és számos, az ütemtervben szereplő összes minta alapján megszámolt téves besorolással rendelkezik.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="0d7cf-110">Hiperparaméterek beállítása</span><span class="sxs-lookup"><span data-stu-id="0d7cf-110">Hyperparameters</span></span>

<span data-ttu-id="0d7cf-111">A modell betanítási folyamatát bizonyos előre beállított, *hiperparaméterek beállítása*nevű értékek szabályozzák:</span><span class="sxs-lookup"><span data-stu-id="0d7cf-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="0d7cf-112">Tanulási sebesség</span><span class="sxs-lookup"><span data-stu-id="0d7cf-112">Learning rate</span></span>

<span data-ttu-id="0d7cf-113">Ez a legfontosabb hiperparaméterek beállítása egyike.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="0d7cf-114">Meghatározza, hogy a sztochasztikus gradiens aktuális becsült értéke milyen mértékben befolyásolja a paraméter frissítését.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="0d7cf-115">A paraméter-frissítési különbözet mérete arányos a képzési aránysal.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="0d7cf-116">A kisebb tanulási arány a lassabb paraméterek alakulását és a lassabb konvergenciát eredményezi, de az LR túlságosan nagy értékei megszüntetik a konvergenciát egészen addig, amíg a színátmenet kivezetése soha nem véglegesíti az adott helyi minimális értéket.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="0d7cf-117">Míg a képzési algoritmus adaptív bizonyos mértékig korrigálva van, a megfelelő kezdeti érték kiválasztása fontos.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="0d7cf-118">A tanulási sebesség szokásos alapértelmezett kezdeti értéke 0,1.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="0d7cf-119">A tanulási sebesség legjobb értékének kiválasztása egy képzőművészet (lásd például a Goodfellow et al., "Deep learning", "do Press", 2017) 4,3 című szakaszt.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="0d7cf-120">Minibatch mérete</span><span class="sxs-lookup"><span data-stu-id="0d7cf-120">Minibatch size</span></span>

<span data-ttu-id="0d7cf-121">Meghatározza, hogy hány adatminta van használatban a sztochasztikus gradiens egyetlen becsléséhez.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="0d7cf-122">A minibatch méretének nagyobb értéke általában robusztusabb és több monoton konvergenciát eredményez, de a betanítási folyamat lelassulhat, mivel a minimatch méretével arányos egy átmenetes becslés díja.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="0d7cf-123">A minibatch szokásos alapértelmezett értéke 10.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="0d7cf-124">Képzési korszakok, tolerancia, gridlocks</span><span class="sxs-lookup"><span data-stu-id="0d7cf-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="0d7cf-125">Az "EPOCH" kifejezés azt jelenti, hogy az ütemezett betanítási adatszolgáltatások egyike teljes.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="0d7cf-126">A betanítási szálon (lásd alább) lévő időpontok maximális számát meg kell szabni.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="0d7cf-127">A betanítási szál a leálláshoz van definiálva (a legismertebb jelölt paraméterekkel), amikor a rendszer elvégezte az időpontok maximális számát.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="0d7cf-128">Az ilyen képzések azonban korábban megszűnnek, ha az érvényesítési ütemterv nem a kiválasztott tűréshatár alá esik.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="0d7cf-129">Tegyük fel például, hogy a téves besorolási tűréshatár 0,01 (1%); Ha az 2000-minták érvényesítési készletében kevesebb, mint 20 téves besorolás látható, akkor a rendszer elérte a tűréshatár szintjét.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="0d7cf-130">Egy betanítási szál idő előtt leáll, ha a pályázó modell ellenőrzési pontszáma nem mutatott javulást több egymást követő korszakban (egy körbetartozás).</span><span class="sxs-lookup"><span data-stu-id="0d7cf-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="0d7cf-131">A körbetartozás-megszakítás logikája jelenleg hardcoded.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="0d7cf-132">Mérések száma</span><span class="sxs-lookup"><span data-stu-id="0d7cf-132">Measurements count</span></span>

<span data-ttu-id="0d7cf-133">A betanítási/érvényesítési pontszámok, valamint a sztochasztikus gradiensnek a kvantum-eszközön lévő összetevőinek becslése a kvantum-állapot átfedésének megbecsléséhez, amely a megfelelő observables több mérését igényli.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="0d7cf-134">A mérések számát $O (1/\ epszilon ^ 2) $ értékre kell méretezni, ahol a $ \epsilon $ a kívánt becslési hiba.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="0d7cf-135">A szabály a kezdeti mérések száma körülbelül $1/\ mbox {tolerancia} ^ 2 $ lehet (lásd a tolerancia definícióját az előző bekezdésben).</span><span class="sxs-lookup"><span data-stu-id="0d7cf-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="0d7cf-136">Az egyiknek újra kell változtatnia a mérések darabszámát, ha úgy tűnik, hogy a gradiens túlságosan kiszámíthatatlan, és a konvergencia túl nehezen elérhető.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="0d7cf-137">Betanítási szálak</span><span class="sxs-lookup"><span data-stu-id="0d7cf-137">Training threads</span></span>

<span data-ttu-id="0d7cf-138">A valószínűségi függvény, amely az osztályozó betanítási segédprogramja, nagyon ritkán domború, ami azt jelenti, hogy általában számos helyi Optima szerepel a paraméterben, amely jelentősen különbözhet a minőségtől.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="0d7cf-139">Mivel az SGD-folyamat csak egy adott optimális értékre konvergál, fontos felderíteni több kezdő paraméteres vektort is.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="0d7cf-140">A gépi tanulás gyakori gyakorlata, hogy az ilyen kezdő vektorokat véletlenszerűen inicializálja.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="0d7cf-141">A Q# betanítási API-k tetszőleges tömböt fogadnak el az ilyen kiindulási vektorokból, de az alapul szolgáló kód szekvenciálisan tárja fel őket.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="0d7cf-142">Egy többplatformos számítógépen vagy a párhuzamos számítástechnikai architektúrával kapcsolatban ajánlott több hívást végrehajtani Q# az API-t párhuzamosan a hívások különböző paramétereinek inicializálásával.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="0d7cf-143">A hiperparaméterek beállítása módosítása</span><span class="sxs-lookup"><span data-stu-id="0d7cf-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="0d7cf-144">A QML-könyvtárban a legjobb módszer a hiperparaméterek beállítása módosítására, ha felülbírálja a UDT alapértelmezett értékeit [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="0d7cf-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="0d7cf-145">Ehhez hívjuk meg a függvényt, [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) és alkalmazzuk az operátort az `w/` alapértelmezett értékek felülbírálására.</span><span class="sxs-lookup"><span data-stu-id="0d7cf-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="0d7cf-146">Például az 100 000 mérések és a 0,01-es tanulási arány használatához:</span><span class="sxs-lookup"><span data-stu-id="0d7cf-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
